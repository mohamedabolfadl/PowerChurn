# -*- coding: utf-8 -*-
"""
Created on Mon Jul 10 16:32:06 2017
Script to demonstrate the gains in revenue when using the optimal discount
@author: m00760171
"""


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import roc_curve, auc

rf_col='navy'
dt_col='m'
lr_col='mediumpurple'
nb_col='darkorchid'
knn_col='plum'
svm_col='blue'
#acceptance_kernel = 'quad'
p_a_m = 5 # relationship between dicount provided and probability of accepting it 20% -> 100% acceptance, hence the slope is 5 


# Acceptance probability of user for a given discount
def acceptProb(d,acceptance_kernel):
    if acceptance_kernel == 'lin':    
        return p_a_m*d
    else:
        return -(1/ (0.2*0.2))*(d-0.2)**2+1

# Plot ROC
def plotROC(y_test, y_score_l,label,col,p,d,plotFlag, kern):
    y_score = np.array(y_score_l)
    y_test = np.array(y_test)
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    fpr, tpr, thresholds = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    x_arr = np.linspace(0.0, 1.0, num=10)
    y_thresh = ((1-p)/p)*(d/(1-d))*x_arr
    lw = 1
    if plotFlag:
        plt.plot(fpr, tpr,
                 lw=lw, label=label + ' (area = %0.2f)' % roc_auc)
        plt.plot(x_arr,y_thresh,lw=1,color='r', linestyle='--',label='Margin '+'(p:'+str(100*p)+'%,d:'+str(100*d)+'%)')
        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.0])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC')
        plt.legend(loc="lower right")
        plt.grid()

    
    
    S_improvement = []
    
    for i in range(0, len(fpr)):
        f = fpr[i]
        t = tpr[i]
        S_improvement.append(100.0*(t*p*(1-d)*min(1,acceptProb(d,kern)) + (1-p)*(1-f) + (1-p)*f*(1-d) - (1-p))/(1-p))
        
    if plotFlag:
        plt.figure()
        plt.plot(thresholds,S_improvement,label=label)
        plt.plot(np.array(thresholds),np.zeros(len(thresholds)),color='r', linestyle='--')
        #plt.plot(np.array(thresholds),np.ones(10))
        plt.xlim([0.0, 1.0])
        plt.xlabel('Threshold')
        plt.ylabel('Improvement in S [%]')
        plt.legend(loc="lower right")
        #plt.title('Effectiveness of model')
        plt.grid()
    return max(S_improvement)
    



# Importing the dataset
dataset = pd.read_csv('ml_case_training_data_cleaned.csv')
slct_inds = [0,12,3,15,21,23,4,26,6,20,2,7,25,16,13,5,19,29,27,10,28,17,1,11,30,24,31,8,18,22,9,34,38,36,33,40,39,32,35,14,37]
N_feat = len(slct_inds)-10 # Select top N_feat features
X = dataset.iloc[:, slct_inds].values
X = X[:,0:N_feat]
y = dataset.iloc[:, 32].values


# Filling NaNs with their mean
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
imputer = imputer.fit(X[:, 1:32])
X[:, 1:32] = imputer.transform(X[:, 1:32]) 

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Random Forest
if True:
    # Fitting Random Forest Classification to the Training set
    from sklearn.ensemble import RandomForestClassifier
    classifier = RandomForestClassifier(n_estimators = 700, criterion = 'entropy', random_state = 0)
    classifier.fit(X_train, y_train)
    y_conf_2 = classifier.predict_proba(X_test)
    y_conf_2 = y_conf_2[:,1]
    y_pred = classifier.predict(X_test)
    from sklearn.metrics import confusion_matrix, accuracy_score
    cm = confusion_matrix(y_test, y_pred)
    name = "RF"

# NN
if False:
    import keras
    from keras.models import Sequential
    from keras.layers import Dense
    classifier = Sequential()
    classifier.add(Dense(units = int(np.floor(2*(N_feat-1)/3)), kernel_initializer = 'uniform', activation = 'relu', input_dim = N_feat))
    classifier.add(Dense(units = int(np.floor(2*(N_feat-1)/3)), kernel_initializer = 'uniform', activation = 'relu'))
    classifier.add(Dense(units = int(np.floor((N_feat-1)/3)), kernel_initializer = 'uniform', activation = 'relu'))
    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.fit(X_train, y_train, batch_size = 5, epochs = 100)
    y_conf_2 = classifier.predict(X_test)
    y_pred = (y_conf_2 > 0.5)
    name = "NN"

#SVM
if False:
    from sklearn.svm import SVC
    classifier = SVC(kernel = 'rbf', random_state = 0, probability = True)
    classifier.fit(X_train, y_train)
    y_conf_2 = classifier.predict_proba(X_test)
    y_pred = classifier.predict(X_test)
    from sklearn.metrics import confusion_matrix, accuracy_score
    cm = confusion_matrix(y_test, y_pred)
    accuracy_score(y_test, y_pred)
    print('SVM accuracy = ' + str(100.0*accuracy_score(y_test, y_pred)))
    plotROC(y_test, y_conf_2[:,1],"SVM",svm_col)
    y_conf_2 = y_conf_2[:,1]
    name = "SVM"



# Linear and quadratic acceptance probability profiles
plt.figure()
plt.plot( np.linspace(0,20,40) ,  acceptProb(np.linspace(0,0.2,40),'lin'), label='Linear kernel'  )
plt.plot( np.linspace(0,20,40) ,  acceptProb(np.linspace(0,0.2,40),'quad')  , label='Quadtratic kernel')
plt.xlabel("Discount [%]")
plt.ylabel("Acceptance probability")
plt.grid()

# Plot the ROC curve
plt.figure()
plotROC(y_test, y_conf_2,name,rf_col,p=0.11,d=0.2,plotFlag = True, kern ='lin')

# Plot the sales improvement for different discounts and churns
plt.figure()
discount_vec = np.linspace(0.01,0.2,num=100)
churn_vec = np.array([0.05,0.1,0.15,0.2])
for j in range(len(churn_vec)):

    improvement_vs_discount = []
    improvement_vs_discount_quad = []
    
    for i in range(len(discount_vec)):
        improvement_vs_discount.append(plotROC(y_test, y_conf_2,name,rf_col,p=churn_vec[j],d=discount_vec[i],plotFlag = False, kern ='lin'))
        improvement_vs_discount_quad.append(plotROC(y_test, y_conf_2,name,rf_col,p=churn_vec[j],d=discount_vec[i],plotFlag = False, kern ='quad'))
    
    plt.plot(100.0*discount_vec,improvement_vs_discount,label = 'Lin = '+str(round(100.0*churn_vec[j]))+' %')
    plt.plot(100.0*discount_vec,improvement_vs_discount_quad,label = 'Quad = '+str(round(100.0*churn_vec[j]))+' %', linestyle = '--')

plt.xlabel("Discount [%]")
plt.ylabel("Sales improvement [%]")
plt.show()
plt.grid()
plt.legend(loc="upper left")  

    
    
    

    
    
    
    
    
