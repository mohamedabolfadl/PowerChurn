# -*- coding: utf-8 -*-
"""
Created on Mon Jul 10 16:32:06 2017

@author: m00760171
"""


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import roc_curve, auc

rf_col='navy'
dt_col='m'
lr_col='mediumpurple'
nb_col='darkorchid'
knn_col='plum'
svm_col='blue'
#p = 0.11 # Fraction of volume who will churn
#d = 0.2  # Discount

#acceptance_kernel = 'quad'
p_a_m = 5 # relationship between dicount provided and probability of accepting it  



def acceptProb(d,acceptance_kernel):
    
    if acceptance_kernel == 'lin':    
        return p_a_m*d
    else:
        return -(1/ (0.2*0.2))*(d-0.2)**2+1

def plotROC(y_test, y_score_l,label,col,p,d,plotFlag, kern):
    
 #   y_score = np.array(y_score_l[:,1])
    y_score = np.array(y_score_l)

    y_test = np.array(y_test)
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    fpr, tpr, thresholds = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    x_arr = np.linspace(0.0, 1.0, num=10)
    y_thresh = ((1-p)/p)*(d/(1-d))*x_arr
    lw = 1
    if plotFlag:
    # , color=col
        #plt.subplot(1,2,1)
        #fig, ax = plt.subplots()
        plt.plot(fpr, tpr,
                 lw=lw, label=label + ' (area = %0.2f)' % roc_auc)
        plt.plot(x_arr,y_thresh,lw=1,color='r', linestyle='--',label='Margin '+'(p:'+str(100*p)+'%,d:'+str(100*d)+'%)')
        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
        #ax.fill_between(x_arr, tpr, y_thresh)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.0])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC')
        plt.legend(loc="lower right")
        plt.grid()

    
    
    S_improvement = []
    
    for i in range(0, len(fpr)):
        f = fpr[i]
        t = tpr[i]
        S_improvement.append(100.0*(t*p*(1-d)*min(1,acceptProb(d,kern)) + (1-p)*(1-f) + (1-p)*f*(1-d) - (1-p))/(1-p))
        
    if plotFlag:
        plt.figure()
        plt.plot(thresholds,S_improvement,label="Random Forest")
        plt.plot(np.array(thresholds),np.zeros(len(thresholds)),color='r', linestyle='--')
        #plt.plot(np.array(thresholds),np.ones(10))
        plt.xlim([0.0, 1.0])
        plt.xlabel('Threshold')
        plt.ylabel('Improvement in S [%]')
        plt.legend(loc="lower right")
        #plt.title('Effectiveness of model')
        plt.grid()
    print("--------------------------")
    print("Discount = "+str(100.0*d)+" %")
    print("Max improvement = "+str(max(S_improvement)))
    return max(S_improvement)
    



# Importing the dataset
dataset = pd.read_csv('ml_case_training_data_cleaned.csv')
X = dataset.iloc[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,33,34,35,36,37,38,39,40,41]].values
y = dataset.iloc[:, 32].values


# Filling NaNs with their mean
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
imputer = imputer.fit(X[:, 1:32])
X[:, 1:32] = imputer.transform(X[:, 1:32]) 

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
#from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#SVM
if False:
    # Fitting Random Forest Classification to the Training set
    from sklearn.ensemble import RandomForestClassifier
    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
    classifier.fit(X_train, y_train)
    #y_conf_1 = classifier.decision_function(X_test)
    y_conf_2 = classifier.predict_proba(X_test)
    y_conf_2 = y_conf_2[:,1]
    # Predicting the Test set results
    y_pred = classifier.predict(X_test)
    # Making the Confusion Matrix
    from sklearn.metrics import confusion_matrix, accuracy_score
    cm = confusion_matrix(y_test, y_pred)
    #print('Random Forest accuracy = ' + str(100.0*accuracy_score(y_test, y_pred)))
    # ------- ROC plot

# NN
if False:
    import keras
    from keras.models import Sequential
    from keras.layers import Dense
    classifier = Sequential()
    classifier.add(Dense(units = 35, kernel_initializer = 'uniform', activation = 'relu', input_dim = 41))
    classifier.add(Dense(units = 35, kernel_initializer = 'uniform', activation = 'relu'))
    classifier.add(Dense(units = 25, kernel_initializer = 'uniform', activation = 'relu'))
    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.fit(X_train, y_train, batch_size = 5, epochs = 100)
    y_conf_2 = classifier.predict(X_test)
    y_pred = (y_conf_2 > 0.5)

#SVM
if True:
    from sklearn.svm import SVC
    classifier = SVC(kernel = 'rbf', random_state = 0, probability = True)
    classifier.fit(X_train, y_train)
    # Getting confideence per user
    y_conf_2 = classifier.predict_proba(X_test)
    # Predicting the Test set results
    y_pred = classifier.predict(X_test)
    # Making the Confusion Matrix
    from sklearn.metrics import confusion_matrix, accuracy_score
    cm = confusion_matrix(y_test, y_pred)
    accuracy_score(y_test, y_pred)
    print('SVM accuracy = ' + str(100.0*accuracy_score(y_test, y_pred)))
    plotROC(y_test, y_conf_2[:,1],"SVM",svm_col)
    y_conf_2 = y_conf_2[:,1]




plt.figure()
plt.plot( np.linspace(0,20,40) ,  acceptProb(np.linspace(0,0.2,40),'lin'), label='Linear kernel'  )
plt.plot( np.linspace(0,20,40) ,  acceptProb(np.linspace(0,0.2,40),'quad')  , label='Quadtratic kernel')
plt.xlabel("Discount [%]")
plt.ylabel("Acceptance probability")
plt.grid()

plt.figure()
plotROC(y_test, y_conf_2,"NN",rf_col,p=0.11,d=0.2,plotFlag = True, kern ='lin')

plt.figure()
discount_vec = np.linspace(0.01,0.2,num=100)
churn_vec = np.array([0.05,0.11,0.15,0.2])
for j in range(len(churn_vec)):

    improvement_vs_discount = []
    improvement_vs_discount_quad = []
    
    for i in range(len(discount_vec)):
        improvement_vs_discount.append(plotROC(y_test, y_conf_2,"SVM",rf_col,p=churn_vec[j],d=discount_vec[i],plotFlag = False, kern ='lin'))
        improvement_vs_discount_quad.append(plotROC(y_test, y_conf_2,"SVM",rf_col,p=churn_vec[j],d=discount_vec[i],plotFlag = False, kern ='quad'))
    
    plt.plot(100.0*discount_vec,improvement_vs_discount,label = 'Lin = '+str(round(100.0*churn_vec[j]))+' %')
    plt.plot(100.0*discount_vec,improvement_vs_discount_quad,label = 'Quad = '+str(round(100.0*churn_vec[j]))+' %', linestyle = '--')

plt.xlabel("Discount [%]")
plt.ylabel("Sales improvement [%]")
plt.show()
plt.grid()
plt.legend(loc="upper left")  
#thresholdMap = np.zeros((100, 100))
#p = 0.08
#d = 0.2
#lim = ((1-p)/p)*(d/(1-d))
#for i in range(1,101):
#    for j in range(1,101):
#         if (j/i)>lim:
#             thresholdMap[100-j,i-1]=1
#plt.imshow(thresholdMap)        





if False:
    lw = 1
    label = "Random Forest "
    # , color=col
    plt.plot(fpr, tpr,
             lw=lw, label=label + ' (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC')
    plt.legend(loc="lower right")    
    
    plt.grid()
    
    plt.show()
    
    
    
    
    
    
    

    
    
    
    
    
